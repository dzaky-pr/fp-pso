name: "🛠 CI Pipeline"

on:
  workflow_dispatch:
  push:
    branches:
      - main

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    name: 🚀 Build & Upload Artifacts
    runs-on: ubuntu-latest

    env:
      AWS_REGION: ap-southeast-1

    steps:
      - name: ⬇ Checkout Code (repository)
        uses: actions/checkout@v4

      - name: ⎔ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: "npm"

      - name: ⛅ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.6

      - name: 🌱 Terraform Init
        run: terraform -chdir=./terraform init
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: 📦 Terraform Output
        id: tf_output
        run: |
          terraform -chdir=./terraform output -json > tf_output.json

          # Mask sensitive values in logs
          CI_ACCESS_KEY=$(jq -r '.ci_access_key_id.value' tf_output.json)
          CI_SECRET_KEY=$(jq -r '.ci_secret_access_key.value' tf_output.json)
          echo "::add-mask::$CI_ACCESS_KEY"
          echo "::add-mask::$CI_SECRET_KEY"

          echo "api_gateway_url=$(jq -r '.api_gateway_url.value' tf_output.json)" >> $GITHUB_OUTPUT
          echo "artifact_bucket=$(jq -r '.artifact_bucket_name.value' tf_output.json)" >> $GITHUB_OUTPUT
          echo "ci_access_key_id=$CI_ACCESS_KEY" >> $GITHUB_OUTPUT
          echo "ci_secret_access_key=$CI_SECRET_KEY" >> $GITHUB_OUTPUT
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: ✅ Check Terraform Output
        run: |
          echo "API Gateway URL: ${{ steps.tf_output.outputs.api_gateway_url }}"
          echo "Artifact Bucket: ${{ steps.tf_output.outputs.artifact_bucket }}"

      - name: 📦 Install Dependencies With Audit
        run: |
          # Install with audit and cache optimization
          npm ci

      - name: 🔍 Lint & Typecheck
        run: |
          npm run lint
          npm run typecheck

      - name: 🧪 Run Unit Tests
        run: |
          npm test -- --verbose

      - name: 🏗 Setup .env
        run: |
          echo "AWS_API_URL=${{ steps.tf_output.outputs.api_gateway_url }}" >> .env

      - name: 🏗 Build Next.js App
        run: npm run build

      - name: 📦 Prepare Package Artifacts
        run: |
          echo "🔍 Analyzing build output..."
          ls -la .next/

          # Create artifact structure
          mkdir -p artifact

          # Check if standalone build exists
          if [ -d ".next/standalone" ]; then
            echo "✅ Found standalone build, copying..."
            
            # Copy standalone build to artifact first (contains .next with BUILD_ID, manifests, etc.)
            cp -r .next/standalone/* artifact/
            
            # Copy additional static files to the static subdirectory (DO NOT overwrite .next root)
            if [ -d ".next/static" ]; then
              # Ensure static directory exists and copy contents
              mkdir -p artifact/.next/static
              cp -r .next/static/* artifact/.next/static/
              echo "✅ Copied static files to artifact/.next/static/"
            else
              echo "⚠️ No static files found"
            fi
            
            # Copy public directory
            cp -r public artifact/ || echo "⚠️ No public directory found"
            
            # Copy package.json and package-lock.json for dependencies (will overwrite standalone ones)
            cp package.json artifact/ || echo "⚠️ No package.json found"
            cp package-lock.json artifact/ || echo "⚠️ No package-lock.json found"
            
            # Ensure server.js is executable
            if [ -f "artifact/server.js" ]; then
              chmod +x artifact/server.js
              echo "✅ Found server.js in artifact root"
            else
              echo "❌ server.js not found in standalone build!"
              exit 1
            fi
            
            echo "🔍 Verifying artifact structure:"
            ls -la artifact/
            echo "🔍 Checking .next directory in artifact:"
            ls -la artifact/.next/ || echo "❌ No .next directory in artifact"
            echo "🔍 Checking static files in .next:"
            ls -la artifact/.next/static/ || echo "❌ No static directory in .next"
            echo "🔍 Checking BUILD_ID in .next:"
            if [ -f "artifact/.next/BUILD_ID" ]; then
              echo "✅ BUILD_ID found: $(cat artifact/.next/BUILD_ID)"
            else
              echo "❌ BUILD_ID missing - this will cause production build error!"
            fi
          else
            echo "❌ No standalone build found - check next.config.mjs for output: 'standalone'"
            echo "Available files in .next:"
            ls -la .next/
            exit 1
          fi

          echo "📂 Final artifact structure:"
          find artifact -type f -name "*.js" -o -name "package.json" | head -10
          echo "📁 Artifact directory size:"
          du -sh artifact

      - name: 📦 Zip Artifacts
        run: |
          cd artifact && zip -r ../artifact.zip . -x "*.log" "*.tmp" "node_modules/*"

      - name: 🔍 Verify Artifact Integrity
        run: |
          # Generate checksum for artifact
          sha256sum artifact.zip > artifact.zip.sha256
          echo "Artifact size: $(ls -lh artifact.zip | awk '{print $5}')"

          # Verify minimum size (should be > 1KB for basic validation)
          ARTIFACT_SIZE=$(stat -f%z artifact.zip 2>/dev/null || stat -c%s artifact.zip)
          if [ "$ARTIFACT_SIZE" -lt 1000 ]; then
            echo "::error::Artifact size too small ($ARTIFACT_SIZE bytes)"
            exit 1
          else
            echo "✅ Artifact size valid: $ARTIFACT_SIZE bytes"
          fi

      - name: ☁ Upload Artifact Zip to S3
        run: |
          aws s3 cp artifact.zip s3://${{ steps.tf_output.outputs.artifact_bucket }}/${{ github.sha }}.zip
        env:
          AWS_ACCESS_KEY_ID: ${{ steps.tf_output.outputs.ci_access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ steps.tf_output.outputs.ci_secret_access_key }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: ☁ Upload Artifact Checksum to S3
        run: |
          aws s3 cp artifact.zip.sha256 s3://${{ steps.tf_output.outputs.artifact_bucket }}/${{ github.sha }}.sha256
        env:
          AWS_ACCESS_KEY_ID: ${{ steps.tf_output.outputs.ci_access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ steps.tf_output.outputs.ci_secret_access_key }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: 📄 Backup latest.txt to previous.txt (safe check)
        run: |
          set +e
          aws s3 cp s3://${{ steps.tf_output.outputs.artifact_bucket }}/latest.txt ./latest.txt

          if [ ! -f latest.txt ]; then
            echo "ℹ️ latest.txt not found. Skipping backup."
            exit 0
          fi

          VERSION=$(cat latest.txt | xargs)

          # Check if the file exists in S3 and has a reasonable size (>1KB)
          FILE_SIZE=$(aws s3api head-object --bucket ${{ steps.tf_output.outputs.artifact_bucket }} --key $VERSION --query ContentLength --output text 2>/dev/null)

          if [[ "$FILE_SIZE" =~ ^[0-9]+$ ]] && [ "$FILE_SIZE" -ge 1000 ]; then
            echo "✅ Valid artifact found: $VERSION ($FILE_SIZE bytes)"
            aws s3 cp ./latest.txt s3://${{ steps.tf_output.outputs.artifact_bucket }}/previous.txt
          else
            echo "::warning::Artifact in latest.txt is missing or too small ($FILE_SIZE bytes). Skipping backup."
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ steps.tf_output.outputs.ci_access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ steps.tf_output.outputs.ci_secret_access_key }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: 📄 Write and Upload latest.txt (safe update)
        run: |
          VERSION="${{ github.sha }}.zip"

          # Pastikan artifact benar-benar ada di S3 dan ukurannya ≥ 1KB
          FILE_SIZE=$(aws s3api head-object \
            --bucket ${{ steps.tf_output.outputs.artifact_bucket }} \
            --key "$VERSION" \
            --query ContentLength \
            --output text 2>/dev/null)

          if [[ "$FILE_SIZE" =~ ^[0-9]+$ ]] && [ "$FILE_SIZE" -ge 1000 ]; then
            echo "✅ Verified uploaded artifact: $VERSION ($FILE_SIZE bytes)"
            
            # Tulis dan upload latest.txt
            echo "$VERSION" > latest.txt
            aws s3 cp latest.txt s3://${{ steps.tf_output.outputs.artifact_bucket }}/latest.txt
            echo "✅ latest.txt now points to $VERSION"
          else
            echo "::error::Artifact $VERSION is missing or too small ($FILE_SIZE bytes). Skipping latest.txt update."
            exit 1
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ steps.tf_output.outputs.ci_access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ steps.tf_output.outputs.ci_secret_access_key }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: 🧹 Clean Up Old Artifacts in S3 (Safe Version)
        run: |
          # Unduh latest.txt dan previous.txt jika ada
          aws s3 cp s3://${{ steps.tf_output.outputs.artifact_bucket }}/latest.txt latest.txt || true
          aws s3 cp s3://${{ steps.tf_output.outputs.artifact_bucket }}/previous.txt previous.txt || true

          # Ambil nama file dari isi latest.txt dan previous.txt
          LATEST=$(cat latest.txt 2>/dev/null | xargs || echo "")
          PREVIOUS=$(cat previous.txt 2>/dev/null | xargs || echo "")

          echo "Found in latest.txt: $LATEST"
          echo "Found in previous.txt: $PREVIOUS"

          # Validasi eksistensi dan ukuran file latest.zip dan previous.zip di S3
          VALID_ZIPS=()

          for ZIPVAR in "$LATEST" "$PREVIOUS"; do
            if [[ "$ZIPVAR" =~ \.zip$ ]]; then
              FILE_SIZE=$(aws s3api head-object --bucket ${{ steps.tf_output.outputs.artifact_bucket }} --key "$ZIPVAR" --query ContentLength --output text 2>/dev/null)
              if [[ "$FILE_SIZE" =~ ^[0-9]+$ ]] && [ "$FILE_SIZE" -ge 1000 ]; then
                echo "✅ Valid artifact: $ZIPVAR ($FILE_SIZE bytes)"
                VALID_ZIPS+=("$ZIPVAR")
              else
                echo "::warning::Skipping invalid or small artifact: $ZIPVAR ($FILE_SIZE bytes)"
              fi
            fi
          done

          echo "Keeping artifacts: ${VALID_ZIPS[*]}"

          # Ambil semua file .zip dari bucket
          aws s3 ls s3://${{ steps.tf_output.outputs.artifact_bucket }}/ | awk '{print $4}' | grep '\.zip$' > all_zips.txt || true

          # Hapus file yang tidak termasuk dalam VALID_ZIPS
          if [ -f all_zips.txt ]; then
            while read ZIPFILE; do
              KEEP=false
              for VALID in "${VALID_ZIPS[@]}"; do
                if [ "$ZIPFILE" = "$VALID" ]; then
                  KEEP=true
                  break
                fi
              done

              if [ "$KEEP" = false ]; then
                echo "🗑 Deleting unused artifact: $ZIPFILE"
                aws s3 rm s3://${{ steps.tf_output.outputs.artifact_bucket }}/$ZIPFILE
              else
                echo "✅ Keeping artifact: $ZIPFILE"
              fi
            done < all_zips.txt
          else
            echo "ℹ️ No .zip files found in bucket."
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ steps.tf_output.outputs.ci_access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ steps.tf_output.outputs.ci_secret_access_key }}
          AWS_REGION: ${{ env.AWS_REGION }}

      - name: 🧹 Clean artifacts and zip
        run: |
          rm -rf artifact artifact.zip
          mkdir artifact
